#!/usr/bin/env python3
"""
NekroAgent æ’ä»¶æ‰¹é‡è¯„ä¼°è„šæœ¬
å¿«é€Ÿè¯„ä¼° 49 ä¸ªæ’ä»¶çš„å„é¡¹æŒ‡æ ‡
"""

import json
import subprocess
import os
from datetime import datetime
from typing import Dict, List

# è¯„ä¼°ç»“æœå­˜å‚¨
EVALUATIONS = []

# åŠ è½½æ’ä»¶åˆ—è¡¨
with open("plugins_complete_data.json", "r", encoding="utf-8") as f:
    plugins = json.load(f)

def clone_and_analyze(plugin: Dict) -> Dict:
    """å…‹éš†å¹¶åˆ†æå•ä¸ªæ’ä»¶"""
    name = plugin.get("name", "æœªçŸ¥")
    module = plugin.get("moduleName", "æœªçŸ¥")
    github = plugin.get("githubUrl", "")
    
    print(f"\n{'='*80}")
    print(f"ğŸ“¦ è¯„ä¼°æ’ä»¶: {name} ({module})")
    print(f"{'='*80}")
    
    eval_result = {
        "name": name,
        "module": module,
        "author": plugin.get("author", "æœªçŸ¥"),
        "github": github,
        "evaluated_at": datetime.now().isoformat(),
        "scores": {},
        "total_score": 0,
        "grade": "F",
        "highlights": [],
        "issues": [],
    }
    
    # å…‹éš†ä»“åº“
    if github and github != "æ— ":
        repo_name = github.replace("https://github.com/", "").rstrip("/")
        local_path = f"/tmp/{repo_name.replace('/', '_')}"
        
        if os.path.exists(local_path):
            print(f"  âœ… å·²å­˜åœ¨: {local_path}")
        else:
            print(f"  ğŸ”„ å…‹éš†ä»“åº“...")
            result = subprocess.run(
                ["git", "clone", "--depth", "1", github, local_path],
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                print(f"  âœ… å…‹éš†æˆåŠŸ")
            else:
                print(f"  âŒ å…‹éš†å¤±è´¥: {result.stderr}")
                eval_result["issues"].append("å…‹éš†å¤±è´¥")
                return eval_result
    
    # åˆ†æä»£ç è´¨é‡
    score = 0
    details = []
    
    # æ£€æŸ¥æ–‡ä»¶
    paths_to_check = [
        f"{local_path}/__init__.py",
        f"{local_path}/plugin.py",
        f"{local_path}/README.md",
        f"{local_path}/pyproject.toml",
        f"{local_path}/LICENSE",
    ]
    
    existing_files = sum(1 for p in paths_to_check if os.path.exists(p))
    score += existing_files * 1.5  # æ¯ä¸ªæ–‡ä»¶ 1.5 åˆ†
    details.append(f"æ–‡ä»¶æ£€æŸ¥: {existing_files}/5")
    
    # åˆ†æä»£ç 
    main_py = f"{local_path}/__init__.py"
    if os.path.exists(main_py):
        try:
            with open(main_py, "r", encoding="utf-8") as f:
                content = f.read()
            
            # æ£€æŸ¥ç±»å‹æ³¨è§£
            if "async def" in content and "->" in content:
                score += 1.5
                details.append("âœ… ç±»å‹æ³¨è§£å®Œæ•´")
            elif "async def" in content:
                score += 0.5
                details.append("âš ï¸ ç¼ºå°‘ç±»å‹æ³¨è§£")
            
            # æ£€æŸ¥é”™è¯¯å¤„ç†
            if "try:" in content and "except" in content:
                score += 1
                details.append("âœ… é”™è¯¯å¤„ç†å®Œå–„")
            
            # æ£€æŸ¥æ—¥å¿—
            if "logger" in content or "logging" in content:
                score += 1
                details.append("âœ… ä½¿ç”¨æ—¥å¿—è®°å½•")
            
            # æ£€æŸ¥æ–‡æ¡£å­—ç¬¦ä¸²
            if '"""' in content or "'''" in content:
                score += 1.5
                details.append("âœ… æ–‡æ¡£å­—ç¬¦ä¸²å®Œæ•´")
            
            # æ£€æŸ¥é…ç½®
            if "ConfigBase" in content or "@plugin.mount_config" in content:
                score += 1.5
                details.append("âœ… é…ç½®ç³»ç»Ÿå®Œæ•´")
            
            # æ£€æŸ¥æ²™ç›’æ–¹æ³•
            if "SandboxMethodType" in content or "mount_sandbox_method" in content:
                score += 1.5
                details.append("âœ… æ²™ç›’æ–¹æ³•å®šä¹‰")
            
            # æ£€æŸ¥æç¤ºè¯æ³¨å…¥
            if "mount_prompt_inject_method" in content:
                score += 1
                details.append("âœ… æç¤ºè¯æ³¨å…¥")
            
        except Exception as e:
            details.append(f"âŒ ä»£ç åˆ†æå¤±è´¥: {e}")
    
    # å½’ä¸€åŒ–ä¸º 10 åˆ†åˆ¶
    normalized_score = min(score / 2, 10)  # æ»¡åˆ†çº¦ 20 åˆ†ï¼Œå½’ä¸€åŒ–ä¸º 10 åˆ†
    eval_result["scores"]["ä»£ç è´¨é‡"] = round(normalized_score, 1)
    
    # åŸºäºä»£ç è´¨é‡ä¼°ç®—å…¶ä»–ç»´åº¦
    base_score = normalized_score
    
    # åŠŸèƒ½å®Œæ•´æ€§ (åŸºäºä»£ç å¤æ‚åº¦)
    eval_result["scores"]["åŠŸèƒ½å®Œæ•´æ€§"] = round(base_score * 0.95, 1)
    
    # æ–‡æ¡£å®Œå–„åº¦ (åŸºäº README)
    readme_path = f"{local_path}/README.md"
    has_readme = os.path.exists(readme_path)
    readme_size = os.path.getsize(readme_path) if has_readme else 0
    doc_score = 10 if (has_readme and readme_size > 2000) else (7 if has_readme else 4)
    eval_result["scores"]["æ–‡æ¡£å®Œå–„åº¦"] = doc_score
    
    # AI ä½¿ç”¨è®¾è®¡ (åŸºäºæ–‡æ¡£å­—ç¬¦ä¸²å’Œæç¤ºè¯)
    ai_score = min(base_score + 1, 10) if "description" in content else base_score
    eval_result["scores"]["AIä½¿ç”¨è®¾è®¡"] = round(ai_score, 1)
    
    # æ˜“ç”¨æ€§
    eval_result["scores"]["æ˜“ç”¨æ€§"] = round(base_score * 0.85, 1)
    
    # å®‰å…¨æ€§ (åŸºäºè¾“å…¥éªŒè¯)
    security_score = base_score if "ValueError" in content or "assert" in content else base_score - 1
    eval_result["scores"]["å®‰å…¨æ€§"] = round(max(security_score, 5), 1)
    
    # ç»´æŠ¤æ´»è·ƒåº¦ (åŸºäº GitHub æ´»åŠ¨)
    try:
        repo = github.replace("https://github.com/", "").rstrip("/")
        result = subprocess.run(
            ["gh", "api", f"/repos/{repo}"],
            capture_output=True, text=True, timeout=10
        )
        if result.returncode == 0:
            data = json.loads(result.stdout)
            updated_at = data.get("updated_at", "")
            if updated_at:
                from datetime import datetime
                update_date = datetime.fromisoformat(updated_at.replace("Z", "+00:00"))
                days_ago = (datetime.now() - update_date).days
                if days_ago < 30:
                    eval_result["scores"]["ç»´æŠ¤æ´»è·ƒåº¦"] = 9.0
                elif days_ago < 90:
                    eval_result["scores"]["ç»´æŠ¤æ´»è·ƒåº¦"] = 7.0
                else:
                    eval_result["scores"]["ç»´æŠ¤æ´»è·ƒåº¦"] = 5.0
            else:
                eval_result["scores"]["ç»´æŠ¤æ´»è·ƒåº¦"] = 6.0
        else:
            eval_result["scores"]["ç»´æŠ¤æ´»è·ƒåº¦"] = 5.0
    except:
        eval_result["scores"]["ç»´æŠ¤æ´»è·ƒåº¦"] = 5.0
    
    # åˆ›æ–°ç¨‹åº¦ (åŸºäºåŠŸèƒ½å’Œæè¿°)
    keywords_innovative = ["è§†é¢‘", "MCP", "åè®®", "åˆ›æ–°", "AI"]
    has_innovative = any(kw in plugin.get("description", "") for kw in keywords_innovative)
    eval_result["scores"]["åˆ›æ–°ç¨‹åº¦"] = 9.0 if has_innovative else 7.0
    
    # ç”¨æˆ·ä½“éªŒ (åŸºäºä»£ç è´¨é‡)
    eval_result["scores"]["ç”¨æˆ·ä½“éªŒ"] = round(base_score * 0.9, 1)
    
    # æ‰©å±•æ€§ (åŸºäºæ¶æ„è®¾è®¡)
    eval_result["scores"]["æ‰©å±•æ€§"] = round(base_score * 0.95, 1)
    
    # è®¡ç®—æ€»åˆ†
    weights = {
        "ä»£ç è´¨é‡": 0.20,
        "åŠŸèƒ½å®Œæ•´æ€§": 0.15,
        "æ–‡æ¡£å®Œå–„åº¦": 0.10,
        "AIä½¿ç”¨è®¾è®¡": 0.15,
        "æ˜“ç”¨æ€§": 0.10,
        "å®‰å…¨æ€§": 0.10,
        "ç»´æŠ¤æ´»è·ƒåº¦": 0.05,
        "åˆ›æ–°ç¨‹åº¦": 0.05,
        "ç”¨æˆ·ä½“éªŒ": 0.05,
        "æ‰©å±•æ€§": 0.05,
    }
    
    total = sum(eval_result["scores"].get(k, 5) * v for k, v in weights.items())
    eval_result["total_score"] = round(total, 1)
    
    # è¯„çº§
    if total >= 90:
        eval_result["grade"] = "A"
    elif total >= 80:
        eval_result["grade"] = "B"
    elif total >= 70:
        eval_result["grade"] = "C"
    elif total >= 60:
        eval_result["grade"] = "D"
    else:
        eval_result["grade"] = "F"
    
    # äº®ç‚¹å’Œé—®é¢˜
    if eval_result["scores"].get("ä»£ç è´¨é‡", 0) >= 8:
        eval_result["highlights"].append("ä»£ç è´¨é‡ä¼˜ç§€")
    if eval_result["scores"].get("AIä½¿ç”¨è®¾è®¡", 0) >= 8:
        eval_result["highlights"].append("AIé›†æˆå‡ºè‰²")
    if eval_result["scores"].get("åˆ›æ–°ç¨‹åº¦", 0) >= 8:
        eval_result["highlights"].append("åŠŸèƒ½åˆ›æ–°")
    
    if eval_result["scores"].get("æ–‡æ¡£å®Œå–„åº¦", 0) < 6:
        eval_result["issues"].append("æ–‡æ¡£éœ€è¦å®Œå–„")
    if eval_result["scores"].get("å®‰å…¨æ€§", 0) < 6:
        eval_result["issues"].append("å®‰å…¨æ€§éœ€è¦åŠ å¼º")
    
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"   æ€»åˆ†: {eval_result['total_score']}/100")
    print(f"   è¯„çº§: {eval_result['grade']}")
    print(f"   äº®ç‚¹: {', '.join(eval_result['highlights']) or 'æ— '}")
    print(f"   é—®é¢˜: {', '.join(eval_result['issues']) or 'æ— '}")
    
    return eval_result

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼° 49 ä¸ªæ’ä»¶...")
    print("=" * 80)
    
    for i, plugin in enumerate(plugins, 1):
        print(f"\n[{i}/{len(plugins)}]")
        try:
            eval_result = clone_and_analyze(plugin)
            EVALUATIONS.append(eval_result)
        except Exception as e:
            print(f"  âŒ è¯„ä¼°å¤±è´¥: {e}")
            EVALUATIONS.append({
                "name": plugin.get("name"),
                "module": plugin.get("moduleName"),
                "error": str(e),
                "total_score": 0,
                "grade": "F",
            })
    
    # ä¿å­˜ç»“æœ
    with open("memory/evaluations/batch_evaluation_results.json", "w", encoding="utf-8") as f:
        json.dump(EVALUATIONS, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    generate_summary_report()
    
    print("\n" + "=" * 80)
    print("âœ… æ‰¹é‡è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: memory/evaluations/batch_evaluation_results.json")

def generate_summary_report():
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    # æŒ‰è¯„åˆ†æ’åº
    sorted_evals = sorted(
        [e for e in EVALUATIONS if "total_score" in e],
        key=lambda x: x["total_score"],
        reverse=True
    )
    
    # ç”Ÿæˆåˆ†çº§ç»Ÿè®¡
    grades = {"A": 0, "B": 0, "C": 0, "D": 0, "F": 0}
    for e in sorted_evals:
        grades[e.get("grade", "F")] = grades.get(e.get("grade", "F"), 0) + 1
    
    # å¹³å‡åˆ†
    valid_scores = [e["total_score"] for e in sorted_evals]
    avg_score = sum(valid_scores) / len(valid_scores) if valid_scores else 0
    
    print("\n" + "=" * 80)
    print("ğŸ“Š è¯„ä¼°æ±‡æ€»ç»Ÿè®¡")
    print("=" * 80)
    print(f"æ€»æ’ä»¶æ•°: {len(EVALUATIONS)}")
    print(f"æœ‰æ•ˆè¯„ä¼°: {len(sorted_evals)}")
    print(f"å¹³å‡åˆ†: {avg_score:.1f}/100")
    print(f"åˆ†çº§ç»Ÿè®¡:")
    for grade, count in sorted(grades.items()):
        bar = "â–ˆ" * count
        print(f"  {grade}: {bar} {count}")

if __name__ == "__main__":
    main()
